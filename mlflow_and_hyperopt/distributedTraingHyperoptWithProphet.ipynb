{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3396f1b4-385b-4daa-bbd5-4f8b789b4a35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Intro\n",
    "\n",
    "The main purpose of this notebook is to show how to use Spark to perform distributed training of ML models on the cluster. Here we'll train a few Prophet models using Hyperopt to tune the hyperparameters. Hyperopt is a general purpose library that can be used to optimise any function that has parameters and here it is used to optimise the prediction loss function (rmse). Additionally,the search of the best hyperparameters will be tracked using MLflow. \n",
    "\n",
    "This notebook uses daily aggregated energy consumption data. \n",
    "\n",
    "Key steps in this notebook:  \n",
    "- Define a train function\n",
    "- Define a search space and select a strategy for the search\n",
    "- Run the optimization and record the search in mlflow\n",
    "\n",
    "Please be aware that this notebook needs to be run on a Databricks cluster with **ML Runtime**. It has been tested with 13.1 ML Runtime.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10bdec5a-621d-44bf-bffe-838ea6d8ce76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e03e47e-17bd-429a-96ab-36878329c4af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "#import hyperopt from the ML runtime. This notebook need to be run on Databricks ML Runtime. \n",
    "#The hyperopt package installed on the ML runtime is different to the open source version. \n",
    "from hyperopt import fmin, hp, tpe\n",
    "from hyperopt import SparkTrials, STATUS_OK\n",
    "from prophet  import Prophet\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d6f77d-3433-46ad-945b-68da4b40cba9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68ab02d5-511b-4b7e-bfc9-26c21e32e730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You must create the expirement by hand in the Databricks Machine Learning Experiments GUI\n",
    "# Then copy the name of the experiment and paste it here\n",
    "\n",
    "experimentPath = \"/Users/simon.buse@ewz.ch/MittelfristPrognoseTest\"\n",
    "\n",
    "if mlflow.get_experiment_by_name(experimentPath) != None:\n",
    "    print(f\"Experiment {experimentPath} exists, setting it as the active experiemnt\")\n",
    "    mlflow.set_experiment(experimentPath)\n",
    "else:\n",
    "    raise Exception(\"You must first create the experiment in the Databricks Machine Learning GUI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6cc4fa6-43ab-4bf7-8ed6-2a91b9a369ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a20dcb9-d6c4-45e9-a5cc-3ac47aa1a5d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def resampleFixEnds(pdf, frequency):\n",
    "    \"\"\"\n",
    "    The function resamples/aggregates the data according to the sampling frequency. Often the first \n",
    "    and last data points will deviate after resampling. As a simple fix, these points are simply deleted\n",
    "    if they deviate more than 20% from their neighboring data point.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf = pdf.resample(frequency).sum(min_count=1)  #frequency: \"D,W,M\"\n",
    "\n",
    "    for column in pdf.columns:\n",
    "        if pdf[column].iloc[0] < 0.8 * pdf[column].iloc[1]:\n",
    "            pdf = pdf.drop(pdf.index[0])\n",
    "\n",
    "        if pdf[column].iloc[-1] < 0.8 * pdf[column].iloc[-2]:\n",
    "            pdf = pdf.drop(pdf.index[-1])\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61f0b7f9-62d7-4045-8a54-dc7e453788fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef5ba4e9-8d13-47f2-8908-8fd0fac7ccc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://data.stadt-zuerich.ch/dataset/ewz_stromabgabe_netzebenen_stadt_zuerich/download/ewz_stromabgabe_netzebenen_stadt_zuerich.csv\"\n",
    "dataPdf = pd.read_csv(url, index_col=None)\n",
    "\n",
    "dataPdf[\"Timestamp\"] = pd.to_datetime(dataPdf[\"Timestamp\"], utc=True)\n",
    "\n",
    "#set timestamp as index to do a daily aggregation\n",
    "dataPdf = dataPdf.set_index(dataPdf[\"Timestamp\"])  \n",
    "dataPdf = resampleFixEnds(dataPdf, \"D\")\n",
    "\n",
    "#Drop the timezone to avoid warnings\n",
    "dataPdf.index = dataPdf.index.tz_localize(None)  \n",
    "\n",
    "#rename the columns into y and ds. needed by prophet\n",
    "dataPdf[\"ds\"] = dataPdf.index\n",
    "#rescaling the data to GWh, good practise not to work with huge numbers\n",
    "dataPdf[\"y\"] = (dataPdf[\"Value_NE5\"].values + dataPdf[\"Value_NE7\"].values)/1e6\n",
    "dataPdf = dataPdf.drop(columns=[\"Value_NE5\", \"Value_NE7\"])\n",
    "\n",
    "# put aside some data for evaluation\n",
    "split = len(dataPdf)-365\n",
    "trainPdf, testPdf = dataPdf.iloc[:split], dataPdf.iloc[split:]\n",
    "trainPdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c32af1e6-a638-4a46-aeaf-bdb4fbeb9064",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define the Model and the Search Space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40318cb6-b200-4293-8b36-df29c226bb05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(params):\n",
    "  \"\"\"\n",
    "  This is our main training function which we pass to Hyperopt.\n",
    "  It takes in hyperparameter settings, fits a model based on those settings,\n",
    "  evaluates the model, and returns the loss.\n",
    "  \"\"\"\n",
    "\n",
    "  with mlflow.start_run(run_name='inner_run', nested=True) as run: \n",
    "    \n",
    "    forecaster = Prophet(\n",
    "        seasonality_mode=        params[\"seasonality_mode\"],\n",
    "        changepoint_prior_scale= params[\"changepoint_prior_scale\"],\n",
    "        seasonality_prior_scale= params[\"seasonality_prior_scale\"],\n",
    "        holidays_prior_scale=    params[\"holidays_prior_scale\"],\n",
    "        changepoint_range=       params[\"changepoint_range\"],\n",
    "    )\n",
    "\n",
    "    if params[\"holidays\"] != None:\n",
    "        forecaster.add_country_holidays(country_name=params[\"holidays\"])\n",
    "\n",
    "    forecaster.fit(trainPdf)\n",
    "    predictedValues = forecaster.predict(testPdf)\n",
    "\n",
    "    rmse = mean_squared_error(y_true=testPdf.y.values, y_pred=predictedValues.yhat.values, squared=False)\n",
    "    \n",
    "    mlflow.log_metric('rmse', rmse)\n",
    "    mlflow.set_tag(\"model\",\"Prophet\")\n",
    "    #mlflow.log_params(params), breaks the code, params are logged automatically.\n",
    "\n",
    "  return {\"loss\": rmse, \"status\": STATUS_OK, \"Trained_Model\": forecaster}\n",
    "\n",
    "# Define the search space for Hyperopt. Prophets main parameters where found here\n",
    "# https://facebook.github.io/prophet/docs/diagnostics.html#hyperparameter-tuning\n",
    "\n",
    "search_space = {\n",
    "  \"seasonality_mode\":        hp.choice(\"seasonality_mode\",[\"multiplicative\", \"additive\"]),\n",
    "  \"holidays\":                hp.choice(\"holidays\",[None,\"Switzerland\"]),\n",
    "  \"changepoint_prior_scale\": hp.loguniform(\"changepoint_prior_scale\", -6.9, -0.69),  # according to recom. same as [0.001,0.5]\n",
    "  \"seasonality_prior_scale\": hp.loguniform(\"seasonality_prior_scale\", -6.9, 2.3),    # according to recom. same as [0.001, 10]\n",
    "  \"holidays_prior_scale\":    hp.loguniform(\"holidays_prior_scale\", -6.9, 2.3),       # according to recom. same as [0.001, 10]\n",
    "  \"changepoint_range\":       hp.uniform(\"changepoint_range\", 0.8, 0.95)              # optional according to docs, default = 0.8\n",
    "}\n",
    "\n",
    "#Give a name to the run, this name is will be used to group the search results.\n",
    "with mlflow.start_run(run_name='outer_run_prophet'):\n",
    "  \n",
    "  # Select a search algorithm for Hyperopt to use.\n",
    "  algorithm = tpe.suggest  # Tree of Parzen Estimators, a Bayesian method\n",
    "\n",
    "  # Distribute tuning across our Spark cluster\n",
    "  sparkTrials = SparkTrials(parallelism=4)\n",
    "\n",
    "  hyperparameters = fmin(\n",
    "      fn=train,\n",
    "      space=search_space,\n",
    "      algo=algorithm,\n",
    "      trials=sparkTrials,\n",
    "      max_evals=30,\n",
    "      timeout=5*60     #seconds\n",
    "      ) \n",
    "\n",
    "  bestModel = sparkTrials.results[np.argmin([r[\"loss\"] for r in sparkTrials.results])][\"Trained_Model\"]\n",
    "\n",
    "  print(hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84990eaf-8065-4771-b0d4-b9dd3db0ed9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f144c3e-b675-4559-85d4-cba5e427bacd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot the outcome of the model\n",
    "f, axes = plt.subplots(2, 1, figsize=(18, 8))\n",
    "\n",
    "predictedValues = bestModel.predict(testPdf)\n",
    "\n",
    "axes[0].plot(predictedValues.ds.values, predictedValues.yhat.values, color=\"tab:red\", label=\"forcast\")\n",
    "#axes[0].plot(trainPdf.ds.values, trainPdf.y.values, color=\"tab:blue\", label=\"train\")\n",
    "axes[0].plot(testPdf.ds.values, testPdf.y.values, color=\"tab:orange\", label=\"truth\", alpha=0.5)\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"NE5 + NE7\")\n",
    "axes[0].set_ylabel(\"Last [GWh]\")\n",
    "\n",
    "xmin, xmax = axes[0].get_xlim()\n",
    "axes[1].plot(testPdf.ds,(predictedValues.yhat.values-testPdf.y.values)/(testPdf.y.values)*100)\n",
    "axes[1].hlines(0, xmin, xmax, color=\"tab:grey\", linestyle=\"--\")\n",
    "axes[1].set_xlim(xmin,xmax)\n",
    "axes[1].set_ylabel(\"(Pred-True)/True [%]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73b6183-463d-4266-80a7-a2eae6c1dbf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot the induvidual components of the best model\n",
    "forecast = bestModel.predict(dataPdf)\n",
    "bestModel.plot(forecast)\n",
    "fig = bestModel.plot_components(forecast)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "distributedTraingHyperoptWithProphet",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
