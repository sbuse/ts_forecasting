{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "#import hyperopt from the ML runtime. This notebook need to be run on Databricks ML Runtime. \n",
    "#The hyperopt package installed on the ML runtime is different to the open source version. \n",
    "from hyperopt import fmin, hp, tpe\n",
    "from hyperopt import SparkTrials, STATUS_OK, Trials\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampleFixEnds(pdf, frequency):\n",
    "    \"\"\"\n",
    "    The function resamples/aggregates the data according to the sampling frequency. Often the first \n",
    "    and last data points will deviate after resampling. As a simple fix, these points are simply deleted\n",
    "    if they deviate more than 20% from their neighboring data point.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf = pdf.resample(frequency).sum(min_count=1)  #frequency: \"D,W,M\"\n",
    "\n",
    "    for column in pdf.columns:\n",
    "        if pdf[column].iloc[0] < 0.8 * pdf[column].iloc[1]:\n",
    "            pdf = pdf.drop(pdf.index[0])\n",
    "\n",
    "        if pdf[column].iloc[-1] < 0.8 * pdf[column].iloc[-2]:\n",
    "            pdf = pdf.drop(pdf.index[-1])\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.stadt-zuerich.ch/dataset/ewz_stromabgabe_netzebenen_stadt_zuerich/download/ewz_stromabgabe_netzebenen_stadt_zuerich.csv\"\n",
    "dataPdf = pd.read_csv(url, index_col=None)\n",
    "\n",
    "dataPdf[\"Timestamp\"] = pd.to_datetime(dataPdf[\"Timestamp\"], utc=True)\n",
    "\n",
    "#set timestamp as index to do a daily aggregation\n",
    "dataPdf = dataPdf.set_index(dataPdf[\"Timestamp\"])  \n",
    "dataPdf = resampleFixEnds(dataPdf, \"D\")\n",
    "\n",
    "#Drop the timezone to avoid warnings\n",
    "dataPdf.index = dataPdf.index.tz_localize(None)  \n",
    "\n",
    "#rename the columns into y and ds. needed by prophet\n",
    "dataPdf[\"ds\"] = dataPdf.index\n",
    "#rescaling the data to GWh, good practise not to work with huge numbers\n",
    "dataPdf[\"y\"] = (dataPdf[\"Value_NE5\"].values + dataPdf[\"Value_NE7\"].values)/1e6\n",
    "dataPdf = dataPdf.drop(columns=[\"Value_NE5\", \"Value_NE7\"])\n",
    "\n",
    "# put aside some data for evaluation\n",
    "split = len(dataPdf)-365\n",
    "trainPdf, testPdf = dataPdf.iloc[:split], dataPdf.iloc[split:]\n",
    "trainPdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params):\n",
    "\n",
    "    model = lbg.LGBMRegressor(**params)\n",
    "    \n",
    "    model.fit(trainPdf)\n",
    "    predictedValues = model.predict(testPdf)\n",
    "\n",
    "    rmse = mean_squared_error(y_true=testPdf.y.values, y_pred=predictedValues.yhat.values, squared=False)\n",
    "    \n",
    "    return {\"loss\": rmse, \"status\": STATUS_OK, \"Trained_Model\": model}\n",
    "\n",
    "search_space = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'lambda_l1': hp.loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': hp.loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': hp.quniform('num_leaves', 2, 256,1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': hp.quniform('bagging_freq', 1, 7,1),\n",
    "        'min_child_samples': hp.quniform('min_child_samples', 5, 100,1),\n",
    "        }\n",
    " \n",
    "\n",
    "\n",
    "# Select a search algorithm for Hyperopt to use.\n",
    "algorithm = tpe.suggest  # Tree of Parzen Estimators, a Bayesian method\n",
    "\n",
    "# Distribute tuning across our Spark cluster\n",
    "#sparkTrials = SparkTrials(parallelism=4)\n",
    "\n",
    "hyperparameters = fmin(\n",
    "    fn=train,\n",
    "    space=search_space,\n",
    "    algo=algorithm,\n",
    "    trials=Trials,\n",
    "    max_evals=30,\n",
    "    timeout=5*60     #seconds\n",
    "    ) \n",
    "\n",
    "bestModel = Trials.results[np.argmin([r[\"loss\"] for r in Trials.results])][\"Trained_Model\"]\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(params):\n",
    "  \"\"\"\n",
    "  This is our main training function which we pass to Hyperopt.\n",
    "  It takes in hyperparameter settings, fits a model based on those settings,\n",
    "  evaluates the model, and returns the loss.\n",
    "  \"\"\"\n",
    "\n",
    "  with mlflow.start_run(run_name='inner_run', nested=True) as run: \n",
    "    \n",
    "    forecaster = Prophet(\n",
    "        seasonality_mode=        params[\"seasonality_mode\"],\n",
    "        changepoint_prior_scale= params[\"changepoint_prior_scale\"],\n",
    "        seasonality_prior_scale= params[\"seasonality_prior_scale\"],\n",
    "        holidays_prior_scale=    params[\"holidays_prior_scale\"],\n",
    "        changepoint_range=       params[\"changepoint_range\"],\n",
    "    )\n",
    "\n",
    "    if params[\"holidays\"] != None:\n",
    "        forecaster.add_country_holidays(country_name=params[\"holidays\"])\n",
    "\n",
    "    forecaster.fit(trainPdf)\n",
    "    predictedValues = forecaster.predict(testPdf)\n",
    "\n",
    "    rmse = mean_squared_error(y_true=testPdf.y.values, y_pred=predictedValues.yhat.values, squared=False)\n",
    "    \n",
    "\n",
    "  return {\"loss\": rmse, \"status\": STATUS_OK, \"Trained_Model\": forecaster}\n",
    "\n",
    "# Define the search space for lbg.LGBMRegressor\n",
    "# https://facebook.github.io/prophet/docs/diagnostics.html#hyperparameter-tuning\n",
    "\n",
    "search_space = {\n",
    "  \"seasonality_mode\":        hp.choice(\"seasonality_mode\",[\"multiplicative\", \"additive\"]),\n",
    "  \"holidays\":                hp.choice(\"holidays\",[None,\"Switzerland\"]),\n",
    "  \"changepoint_prior_scale\": hp.loguniform(\"changepoint_prior_scale\", -6.9, -0.69),  # according to recom. same as [0.001,0.5]\n",
    "  \"seasonality_prior_scale\": hp.loguniform(\"seasonality_prior_scale\", -6.9, 2.3),    # according to recom. same as [0.001, 10]\n",
    "  \"holidays_prior_scale\":    hp.loguniform(\"holidays_prior_scale\", -6.9, 2.3),       # according to recom. same as [0.001, 10]\n",
    "  \"changepoint_range\":       hp.uniform(\"changepoint_range\", 0.8, 0.95)              # optional according to docs, default = 0.8\n",
    "}\n",
    "\n",
    "#Give a name to the run, this name is will be used to group the search results.\n",
    "with mlflow.start_run(run_name='outer_run_prophet'):\n",
    "  \n",
    "  # Select a search algorithm for Hyperopt to use.\n",
    "  algorithm = tpe.suggest  # Tree of Parzen Estimators, a Bayesian method\n",
    "\n",
    "  # Distribute tuning across our Spark cluster\n",
    "  sparkTrials = SparkTrials(parallelism=4)\n",
    "\n",
    "  hyperparameters = fmin(\n",
    "      fn=train,\n",
    "      space=search_space,\n",
    "      algo=algorithm,\n",
    "      trials=sparkTrials,\n",
    "      max_evals=30,\n",
    "      timeout=5*60     #seconds\n",
    "      ) \n",
    "\n",
    "  bestModel = sparkTrials.results[np.argmin([r[\"loss\"] for r in sparkTrials.results])][\"Trained_Model\"]\n",
    "\n",
    "  print(hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
